{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoiberg-2.0 Bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure my RAM doesn't explode, clears all variables in memory\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small percentage of data to speed up tests !\n",
    "PERCENTAGE_OF_DATA = 5 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./mnist_train.csv', header=None)\n",
    "df_test = pd.read_csv('./mnist_test.csv', header=None)\n",
    "df_train = df_train.iloc[:int(df_train.shape[0] * PERCENTAGE_OF_DATA), :]\n",
    "df_test = df_test.iloc[:int(df_test.shape[0] * PERCENTAGE_OF_DATA), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train set shape: {df_train.shape}')\n",
    "print(f'Test set shape: {df_test.shape}')\n",
    "df_train.head()\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={0: 'digit'})\n",
    "df_test = df_test.rename(columns={0: 'digit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['digit'], label='Train set')\n",
    "plt.hist(df_test['digit'], label='Test set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(n_image):\n",
    "    pixels = df_train.drop('digit', axis=1).iloc[n_image].values\n",
    "    img = np.reshape(pixels, (28, 28))\n",
    "    plt.imshow(img, cmap='plasma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['digit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    df_train['digit'].value_counts().sort_index().rename('Train'),\n",
    "    df_test['digit'].value_counts().sort_index().rename('Test')\n",
    "],\n",
    "          axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(df, df_name, fig_name, savefig=False):\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    hist = sns.countplot(data=df, x='digit', saturation=1, color='lightblue')\n",
    "\n",
    "    patch_h = [patch.get_height() for patch in hist.patches]\n",
    "    # patch_h contains the heights of all the patches now\n",
    "\n",
    "    # np.argmax return the index of largest value of the list\n",
    "    idx_tallest = np.argmax(patch_h)\n",
    "    idx_smallest = np.argmin(patch_h)\n",
    "\n",
    "    hist.patches[idx_tallest].set_facecolor('orangered')\n",
    "    hist.patches[idx_smallest].set_facecolor('gold')\n",
    "    hist.bar_label(hist.containers[0], padding=-30)\n",
    "    hist.set_title(df_name)\n",
    "    hist.set_frame_on(False)\n",
    "    hist.get_yaxis().set_visible(False)\n",
    "    hist.set_xlabel('')\n",
    "\n",
    "    if savefig:\n",
    "        fig.add_axes(hist)\n",
    "        fig.savefig(f'graphs/{fig_name}.png', facecolor='white')\n",
    "\n",
    "    plt.rcParams.update({'font.size': 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(df_train, 'Distribution of digits in training dataset',\n",
    "                       'digit_distribution_train_set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the digits are pretty much envenly distributed throughout the training dataset. What about the test set ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distribution(df_test, 'Distribution of digits in test dataset',\n",
    "                       'digit_distribution_test_set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that values in the test set are also evenly distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve, learning_curve, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['digit']\n",
    "X_train = df_train.drop('digit', axis=1)\n",
    "y_test = df_test['digit']\n",
    "X_test = df_test.drop('digit', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score: {model.score(X_train, y_train)}')\n",
    "print(f'Test score: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "k = np.arange(1, 50)\n",
    "\n",
    "# WARNING: this line takes forever to compile, maybe reduce the sample size to speed up the process ?\n",
    "train_score, val_score = validation_curve(model,\n",
    "                                          X_train,\n",
    "                                          y_train,\n",
    "                                          param_name='n_neighbors',\n",
    "                                          param_range=k,\n",
    "                                          cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score_mean = val_score.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Validation curves', fontsize=22)\n",
    "plt.box(False)\n",
    "plt.plot(k, val_score_mean, label='validation')\n",
    "plt.plot(k, train_score.mean(axis=1), label='train')\n",
    "plt.axhline(val_score_mean.max(), c='red', linestyle='--')\n",
    "plt.axvline(val_score_mean.argmax() + 1, c='red', linestyle='--')\n",
    "plt.scatter(val_score_mean.argmax() + 1,\n",
    "            val_score_mean.max(),\n",
    "            c='red',\n",
    "            linewidths=5)\n",
    "plt.xticks(k)\n",
    "\n",
    "plt.xlabel('n_neighbors', fontsize=16)\n",
    "plt.ylabel('score', fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "# plt.savefig('graphs/validation_curves.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(7)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, interesting to see that changing n_neighbors actually **decreased** our accuracy. But it's not so surprinsing when looking at the validation curves (KNN default value for n_neighbors is 5 and we can see on the curves that the score for n_neighbors=5 and n_neighbors=7 are really close)\n",
    "\n",
    "Let's see if taking into account all of the data is useful through learning curves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, train_score, val_score = learning_curve(model,\n",
    "                                           X_train,\n",
    "                                           y_train,\n",
    "                                           train_sizes=np.linspace(.1, 1, 10),\n",
    "                                           cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = plt.subplot()\n",
    "\n",
    "ax.plot(np.linspace(.1, 1, 10), val_score.mean(axis=1), label='validation')\n",
    "ax.plot(np.linspace(.1, 1, 10), train_score.mean(axis=1), label='train')\n",
    "\n",
    "ax.set_title('Learning curves', fontsize=22)\n",
    "ax.set_xlabel('train_sizes', fontsize=16)\n",
    "ax.set_ylabel('accuracy', fontsize=16)\n",
    "ax.set_xticks(np.linspace(.1, 1, 10))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.legend(fontsize=16)\n",
    "\n",
    "ax.set_frame_on(False)\n",
    "ax.grid()\n",
    "\n",
    "# fig.savefig('graphs/learning_curves.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like we are in a situation of overfitting : the more data the better the precision of our model\n",
    "\n",
    "Let's see if changing hyperparameters of KNN does improve it's accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': np.arange(3, 10),\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski', 'cosine']\n",
    "}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GridSearchCV Best parameters: {grid.best_params_}')\n",
    "best_estimator = grid.best_estimator_\n",
    "print(f'Best estimator score: {best_estimator.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually a pretty significant upgrade in accuracy !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"./assets/accuracy.png\" alt=\"Accuracy\" width=728 height=454>\n",
    "  <h4><i>Accuracy definition</i></h4>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. _Why use a separate dataset to measure the performance of an algorithm?<br> What are the results you get when you test your algorithm on the same dataset used in training?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to separate the dataset into two groups:\n",
    "\n",
    "- the **train** set, to train our model\n",
    "- the **test** set, to evaluate the performance of our model\n",
    "\n",
    "If we don't do that and test our algorithm on the same dataset used in training, we would get an inaccurate evaluation of the performance of our model, since it has already seen all of the data. The idea behind the test set is to confront and evaluate our model on real life data, or at least on data that the model had not seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. _What are bias and variance?<br> What do they measure?<br> Which values should they take?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias** is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "\n",
    "**Variance** is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "<center>\n",
    "  <img src=\"./assets/bias_and_variance.png\" alt=\"Bias and variance\">\n",
    "  <h4><i>Bias and variance</i></h4>\n",
    "  <img src=\"./assets/bias_variance_balance.png\" alt=\"Bias and variance balance\">\n",
    "  <h4><i>Bias and variance balance</i></h4>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _3. What is cross validation?<br>What are the main advantages?<br>When can I use it?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a statistical technique for assessing the effectiveness of machine learning models, particularly in cases where we need to mitigate overfitting. It is also of use in determining the hyper parameters of machine learning models, in the sense that which parameters will result in lowest test error.\n",
    "\n",
    "The only possible drawback of this method is that as we gain robustness by increasing the number of splits, we also have to train more model — a potentially tedious and expensive process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _4. Can you explain why it’s important to normalize (i.e. scale) the data when using algorithms like KNN?<br>Is it necessary in our specific study case?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a distance based algorithms. KNN chooses the k closest neighbors and then based on these neighbors, assigns a class (for classification problems) or predicts a value (for regression problems) for a new observation.\n",
    "\n",
    "All such distance based algorithms are affected by the scale of the variables which is why it is always advisable to bring all the features to the same scale **before** applying distance based algorithms like KNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's try to preprocess our data before using the KNN algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the preprocessing had no effect on our model :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. _When you reshaped your image, do you think the order of the columns (that means the order of the pixels) had an importance for the performance of your algorithm?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train.values.reshape(X_train.shape[0], 28, 28)[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reversed = X_train.values[:, ::-1]\n",
    "X_test_reversed = X_test.values[:, ::-1]\n",
    "plt.imshow(X_train_reversed.reshape(X_train_reversed.shape[0], 28, 28)[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_reversed, y_train)\n",
    "model.score(X_test_reversed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as we apply the **same** transformation to both the train and the test set on all of their images, it seems that it does **not** affect the performance of our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. _Which metrics measure performance?<br>What does accuracy tell you?<br>Does accuracy penalize more one mistake over another?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}')\n",
    "    print(\n",
    "        f'Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}'\n",
    "    )\n",
    "    print(\n",
    "        f'Median Absolute Error: {np.sqrt(median_absolute_error(y_test, y_pred))}'\n",
    "    )\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred, average='weighted')}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"DIFFERENT PERFORMANCE METRICS FOR BASIC KNN MODEL\\n{'-'*20}\")\n",
    "evaluate_model(model)\n",
    "print(\n",
    "    f\"DIFFERENT PERFORMANCE METRICS FOR KNN MODEL AFTER GRIDSEARCHCV\\n{'-'*20}\"\n",
    ")\n",
    "evaluate_model(best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of number of correct predictions to the total number of input samples. It works well only if there are equal number of samples belonging to each class, as it does not penalizeke over another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. _Can you give an explicit example where accuracy would not be a relevant metric?<br>In that extreme case, can you propose a more suitable metric?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n",
    "\n",
    "When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy is great, but gives us the false sense of achieving high accuracy.\n",
    "\n",
    "The real problem arises, when the cost of misclassification of the minor class samples are very high. If we deal with a rare but fatal disease, the cost of failing to diagnose the disease of a sick person is much higher than the cost of sending a healthy person to more tests.\n",
    "\n",
    "Logarithmic Loss or **Log Loss**, works by **penalising the false classifications**. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. In general, minimising Log Loss gives greater accuracy for the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. _Do confusion matrix display all informations of algorithms’ learning?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model. It forms the basis for the other types of metrics (Precision, Recall, F1, ROC, AUC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. _Do you want to have a walk in the forest and observe tree growth?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum.. Sure? I guess ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_estimator.predict(X_test)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_multi_matrix = multilabel_confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(2,\n",
    "                       5,\n",
    "                       sharex=True,\n",
    "                       sharey=True,\n",
    "                       figsize=(24, 8),\n",
    "                       gridspec_kw={\n",
    "                           'wspace': .2,\n",
    "                           'hspace': .3\n",
    "                       })\n",
    "for i, matrix in enumerate(cf_multi_matrix):\n",
    "    index = (0, i) if i < 5 else (1, i - 5)\n",
    "    sns.heatmap(data=matrix, ax=ax[index], annot=True, fmt='3', cmap='mako')\n",
    "    ax[index].set_title(f'Digit {i}\\n', fontsize=14)\n",
    "    ax[index].xaxis.set_ticklabels(['False', 'True'])\n",
    "    ax[index].yaxis.set_ticklabels(['False', 'True'])\n",
    "fig.savefig('graphs/multilabel_confusion_matrix.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_arr = np.array([(matrix[1, 1]) / (np.sum(matrix) - matrix[0, 0])\n",
    "                         for matrix in cf_multi_matrix])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.barplot(x=np.arange(accuracy_arr.size),\n",
    "            y=accuracy_arr * 100,\n",
    "            ax=ax,\n",
    "            color='lightblue')\n",
    "\n",
    "patch_h = [patch.get_height() for patch in ax.patches]\n",
    "idx_tallest = np.argmax(patch_h)\n",
    "idx_smallest = np.argmin(patch_h)\n",
    "\n",
    "ax.patches[idx_tallest].set_facecolor('lightgreen')\n",
    "ax.patches[idx_smallest].set_facecolor('orangered')\n",
    "ax.bar_label(ax.containers[0], padding=-30, fmt='%.0f%%', fontsize=14)\n",
    "ax.set_title('Accuracy per digit', fontsize=22)\n",
    "\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(np.arange(accuracy_arr.size), fontsize=16)\n",
    "\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "fig.savefig('graphs/accuracy_per_digit.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.heatmap(data=cf_matrix,\n",
    "            ax=ax,\n",
    "            annot=True,\n",
    "            cmap='mako',\n",
    "            annot_kws={'size': 13})\n",
    "ax.set_title('KNN confusion matrix\\n', fontsize=22)\n",
    "ax.set_xlabel('Predicted Values', fontsize=16)\n",
    "ax.set_ylabel('Actual Values ', fontsize=16)\n",
    "fig.savefig('graphs/KNN_confusion_matrix.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.heatmap(cf_matrix / np.sum(cf_matrix),\n",
    "            annot=True,\n",
    "            fmt='.1%',\n",
    "            cmap='mako',\n",
    "            annot_kws={'size': 13})\n",
    "ax.set_title('KNN confusion matrix (in %)\\n', fontsize=22)\n",
    "ax.set_xlabel('Predicted Values', fontsize=16)\n",
    "ax.set_ylabel('Actual Values ', fontsize=16)\n",
    "fig.savefig('graphs/KNN_confusion_matrix_percentage.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSUPERVISED LEARNING\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=10)\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = [KMeans(n_clusters=k).fit(X_train).inertia_ for k in range(1, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 20), inertia)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest()\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = model.predict(X_train) == -1\n",
    "no_outliers = model.predict(X_train) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train[no_outliers]\n",
    "y_train_filtered = y_train[no_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_filtered, y_train_filtered)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe slightly better results when removing outliers from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "X_train_filtered = model.fit_transform(X_train)\n",
    "X_train_filtered = pd.DataFrame(X_train_filtered, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.scatterplot(data=X_train_filtered,\n",
    "                     x='x',\n",
    "                     y='y',\n",
    "                     hue=y_train,\n",
    "                     palette=sns.color_palette('husl', 10))\n",
    "ax.set_title('Digits 2D visualization (PCA)', fontsize=22)\n",
    "ax.set_axis_off()\n",
    "# plt.savefig('graphs/digits_2d_visualization_PCA.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(perplexity=50)\n",
    "X_train_filtered = model.fit_transform(X_train)\n",
    "X_train_filtered = pd.DataFrame(X_train_filtered, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.scatterplot(data=X_train_filtered,\n",
    "                     x='x',\n",
    "                     y='y',\n",
    "                     hue=y_train,\n",
    "                     palette=sns.color_palette('husl', 10))\n",
    "ax.set_title('Digits 2D visualization (TSNE)', fontsize=22)\n",
    "ax.set_axis_off()\n",
    "# plt.savefig('graphs/digits_2d_visualization_TSNE.png', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=X_train.shape[1])\n",
    "X_train_filtered = model.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=.95)\n",
    "X_train_filtered = model.fit_transform(X_train)\n",
    "model.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_recovered = model.inverse_transform(X_train_filtered)\n",
    "plt.imshow(X_train_recovered[1].reshape(28, 28), cmap='plasma')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_recovered, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    make_pipeline(MinMaxScaler(), PCA(n_components=k),\n",
    "                  KNeighborsClassifier()).fit(X_train,\n",
    "                                              y_train).score(X_test, y_test)\n",
    "    for k in np.arange(0.1, 1, .01)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(scores), np.arange(0.1, 1, .01)[np.argmax(scores)])\n",
    "plt.plot(np.arange(0.1, 1, .01), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(MinMaxScaler(), PCA(),\n",
    "                      KNeighborsClassifier())\n",
    "param_grid = {\n",
    "    'pca__n_components': np.arange(0.95, 1, .01),\n",
    "    'kneighborsclassifier__n_neighbors': np.arange(2, 11),\n",
    "    'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski', 'cosine']\n",
    "}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GridSearchCV Best parameters: {grid.best_params_}')\n",
    "print(f'Best estimator score: {grid.best_estimator_.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train[no_outliers]\n",
    "y_train_filtered = y_train[no_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(MinMaxScaler(), PCA(),\n",
    "                      KNeighborsClassifier())\n",
    "param_grid = {\n",
    "    'pca__n_components': np.arange(0.95, 1, .01),\n",
    "    'kneighborsclassifier__n_neighbors': np.arange(2, 11),\n",
    "    'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski', 'cosine']\n",
    "}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X_train_filtered, y_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GridSearchCV Best parameters: {grid.best_params_}')\n",
    "print(f'Best estimator score: {grid.best_estimator_.score(X_test, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0493f6f68b081c354295624c1c3a8509ce8639884d5649a1929009cc8f42e357"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('zoidberg-bootstrap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
